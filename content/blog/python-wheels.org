#+TITLE: What's the deal with Python wheels?
#+DATE: <2021-09-06 Mon>
#+AUTHOR: Thibault Lestang
#+PROPERTY: LANGUAGE en
#+PROPERTY: STATUS draft
#+PROPERTY: TAGS Python Packaging Wheel
#+OPTIONS: toc:nil

Python wheels are at the heart of Python packaging. If you've ever
installed a package with ~pip~ from the Python Packaging Index, there
is a high chance that it downloaded a wheel file.

#+begin_example
$ pip install numpy
Collecting numpy
  Downloading numpy-1.21.2-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.8 MB)
     |████████████████████████████████| 15.8 MB 65 kB/s
Installing collected packages: numpy
Successfully installed numpy-1.21.2
#+end_example

In the example above the file
~numpy-1.21.2-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl~
is a Python wheel. It contains everything needed to install NumPy 1.21
on my GNU/Linux 64 bits system install my current Python 3.9
environment. In Python, wheels play the role of what we would call a
package in some other contexts (e.g. ~.rpm~ or ~.deb~ package
installation on GNU/Linux). But in Python parlance the word package is
already taken. In Python we say that wheels are /distribution
packages/.

This walktrhough is divided into two main sections

1. Building a wheel for an example package. This is relevant to
   GNU/Linux, MacOS and Windows.
2. Building wheels for GNU/Linux. In these case a bit more work is
   required in order to ensure wheels are portable across /manylinux/
   distributions.

* Example package

  Let's consider an example package with the following layout.

  

  The example package consists of two main files:
  - A C++ extension module ~src/cpp/ddot.cpp~ named ~example~. This
    extension relies on the ~cblas_ddot~ function from an (external)
    CBLAS library. This module turns into a Python module
    ~ddot.cpython-39-x86_64-linux-gnu.so~ with a runtime (dynamic)
    dependency on a CBLAS library (/e.g./ OpenBLAS).
  - A Python file ~dot_product.py~ that imports the ~ddot~ extension module.

  Package build and installation:
  - The extension module is written using ~pybind11~ which a C++
    library that makes it easy interface Python and C++.
  - I used CMake to manage the build simply because CMake works well
    with pybind11 (and this case is well documented).
  - Overall, the Python package is built with ~scikit-build~ which is
    a extension of ~setuptools~ that makes it easy to use CMake as
    part of the setup process.

* Source distributions

  Let's pretend that we want to share this package with a friend.

  A first option is to put everything in an archive:
  #+begin_src shell
	tar -cf septembrse-0.0.0.tar.gz \
	    pyproject.toml \
	    setup.py \
	    CMakeLists.txt \
	    pybind11 \
	    src
  #+end_src

  This is actualy better done automatically using the ~setuptools~ package.
  #+begin_src shell
    python setup.py sdist
  #+end_src

  This creates a ~source distribution~, that contains all the files
  necessary to build and install the package. It also embarks some
  metadata (~PKG-INFO~). Lots of formats available:

  #+begin_src shell :results output
    python setup.py sdist --help-formats
  #+end_src

  #+RESULTS:
  : List of available source distribution formats:
  :   --formats=bztar  bzip2'ed tar-file
  :   --formats=gztar  gzip'ed tar-file
  :   --formats=tar    uncompressed tar file
  :   --formats=xztar  xz'ed tar-file
  :   --formats=zip    ZIP file
  :   --formats=ztar   compressed tar file
  

  Easy enough. We bundled our package's sources into a archive and now
  we can share it with our friend so they can install with ~pip~ in the
  same way.

  #+begin_src shell
    pip install dist/septembrse-0.0.0.tar.gz
  #+end_src

  This unpack the archive and runs the ~setup.py~ script. It
  1. Installs the build tools (~setuptools~, ~cmake~, ~scikit-build~)
  2. Writes metadata
  3. Compiles the extension module
  4. Bundles everything together and copies it to the right
     install location (likely the local /site-packages/ directory).

  Let's see what happens on our friend's machine:

  #+begin_example
    friend@machine:~$ pip install dist/septembrse-0.0.0.tar.gz
    Processing ./dist/septembrse-0.0.0.tar.gz
      ERROR: Command errored out with exit status 1:
      ...
      Complete output (127 lines):
      ... 
      -- Could NOT find BLAS (missing: BLAS_LIBRARIES)
      -- Configuring done
      CMake Error: The following variables are used in this project, but they are set to NOTFOUND.
  #+end_example

  The package cannot be installed since our freind don't have a BLAS
  library installed! After some time looking things up, they firgure
  out that they can
  
  #+begin_src shell
    apt install libopenblas-dev
  #+end_src

  and things run fine.

  
  A ~sdist~ is an archive containing the sources of your package. If
  you give an ~sdist~ to someone, you expect them to be able to
  /build/ the package from scratch. This means meeting all the
  requirements to run the ~setup.py~ script and compiling any
  extension modules.

  In some cases (like the one above), users may get get away with
  installing a package from their system's package manager. Fine. But
  in general, this is a real problem:

  - If there are many dependencies, users have to install them all. Tedious.
  - This is assuming that all these dependencies their is a package
    ready in the system's package repository. Otherwise they will have
    to build the dependency themselves. Tedious, potentially
    technically challenging, and time consuming.
  - This is also assuming that the /right version/ of the dependency
    is packaged. This is especially a problem on GNU/Linux, since
    different distributions package different versions of software. If
    the extension module for our package with linked against version
    ~x~ of a dependency, it's not garanteed to work with version
    ~y~. Frustrating.



  On top of this, remember that when installing a package from a
  ~sdist~, the ~setup.py~ script is executed. This script can actually
  contain any valid Python in addition to the call to
  ~setuptools.setup()~. This means anyone installing a source
  distribution with superuser privileges should feel a bit
  uncomfortable.

* Building a wheel distribution

  What if we gave our friend an archive, but containing an /built/
  version of our package? In this case ~pip~'s only job would be to
  extract the files and copy them in the right place on out friend's computer.

  That's precisely the idea behind Python wheels, and more generally
  behing /built distributions/ (as opposed to /source distributions/).

  A wheel is built using ~setuptools~ with the ~wheel~ package.

  #+begin_src shell
    python setup.py bdist_wheel
  #+end_src

  Let's look at what is happening under the hood.

  1. We start with some CMake output. The C++ extension module
     ~example~ is compiled.

     #+begin_example
       -- Build files have been written to: /home/thibault/repos/septembrse_example/_skbuild/linux-x86_64-3.9/cmake-build
       [ 50%] Building CXX object CMakeFiles/example.dir/src/cpp/ddot.cpp.o
       [100%] Linking CXX shared module example.cpython-39-x86_64-linux-gnu.so
       [100%] Built target example
     #+end_example

   2. Then there is a succesion of creating temporary directories and
	copying files around. Ultimately, both the the Python source
	files and the compiled extension are copied into the ~wheel/~
	directory.

   3. The third step is writing some metadata about the package itself
      and its build. So far this is exactly what happens when you
      install a Python package. Except this time it is install into
      this temporary ~wheel/~ directory instead of your local package
      directory.

   4. The final step is taking the content of that ~wheel~ direcotry
      and putting its content into a zip archive. The ~wheel~
      directory is removed.

  What we learn from this is that a wheel distribution is a zip
  archive that contains the installed form of a Python package.
  #+begin_example
    unzip -l dist/septembrse-0.0.0-cp39-cp39-linux_x86_64.whl
    Archive:  dist/septembrse-0.0.0-cp39-cp39-linux_x86_64.whl
      Length      Date    Time    Name
    ---------  ---------- -----   ----
	   15  2021-09-04 09:02   example_pkg/__init__.py
	93400  2021-09-04 09:02   example_pkg/example.cpython-39-x86_64-linux-gnu.so
	  241  2021-09-04 09:02   septembrse-0.0.0.dist-info/METADATA
	   97  2021-09-04 09:02   septembrse-0.0.0.dist-info/WHEEL
	   12  2021-09-04 09:02   septembrse-0.0.0.dist-info/top_level.txt
	  495  2021-09-04 09:02   septembrse-0.0.0.dist-info/RECORD
    ---------                     -------
	94260                     6 files

  #+end_example

  Now when you install this wheel, all ~pip~ has to do is copy the
  files into your local package directory. It doesn't have to run the
  ~setup.py~ script - all the hard work is already done. Notice that
  the ~setup.py~ script isn't even contained inside the wheel.

  The immediate benefit of this is that I can now share my package
  with my friends, and they don't have to worry about building it.
  They don't have to worry about setuptools, scikit-build, CMake,
  OpenBLAS. They just ~pip install~ and use the package. Its also a
  smaller file to share.


  The downside is that - if my wheel contains compiled code - it is
  platform and python version specific. If its purely Python, I can
  share it with the world and don't worry about portability. If it's
  not, then I'm going to have to build the wheel for each plaftform,
  for each python version I want to support.


* manylinux

  Now let's focus on the case of GNU/Linux, for which there is an
  added complication.  On GNU/Linux, virtually every executable or
  shared library has a dynamic dependency on the GNU standard c
  library (/glibc/). This library is responsible for interfacing with
  the Linux kernel. When you run code from a compiled Python
  extension, this extension is expecting to be able to find some
  symbols (constants, functions) in glibc shared library.

  Now, newer glibc versions are not garanteed to work with older ones.
  This is a problem because different GNU/Linux distros come with the
  different versions of /glibc/. If you build your wheel the latest
  Ubuntu, it's unlikely to work on an older distribution with an older
  glibc.

  The opposite, however, is true. It is garanteed that code linked
  against an older version of glibc wilkl work with a newer one.  So
  in order to build portable wheels, that work on /manylinux/
  distributions, we want to be doing so on older systems that ship
  with older versions of glibc.

  All of this is irrelevant if your package is purely Python code.
  But if you do have one or more extension modules, you need to think
  about that.

  As a Python packager, you don't have to find and install an old
  CentOS 5 image in order to build your wheels. There is a group of
  people named the [[https://www.pypa.io/en/latest/][Python Packaging Authority]] who maintains a set of
  docker images you can use to build your wheels inside.

  We don't have to it ourselves. They're is a group of people named
  ]] that take care of maintaining
  projects used in Python packaging (e.g. PyPI). They maintain [[https://github.com/pypa/manylinux#docker-images][a set
  of Docker images]] you can use to buiold your wheels in.

  For instance the ~manylinux2014~ image is based on CentOS 7:

  #+begin_src shell
    docker run -i -t -v `pwd`:/io quay.io/pypa/manylinux2014_x86_64 /bin/bash
  #+end_src

  Manylinux images contain all currently supported Python versions:
  #+begin_src shell
    root@221b30d4d160:/# ls /opt/python
    cp310-cp310  cp36-cp36m  cp37-cp37m  cp38-cp38	cp39-cp39  pp37-pypy37_pp73
  #+end_src

  Let's build our wheel for, say, Python 3.8. First we install
  OpenBLAS (required to build the C++ extension module).
  #+begin_src shell
    apt update && apt install libopenblas-dev
  #+end_src

  We then build the wheel
  #+begin_src shell
    root@221b30d4d160:/# cd /io/
    root@221b30d4d160:/io# /opt/python/cp38-cp38/bin/pip wheel .
  #+end_src

  #+begin_src shell
    root@221b30d4d160:/io# ls -l | grep .whl$
    -rw-r--r-- 1 root root   42127 Sep  3 09:58 septembrse-0.0.0-cp38-cp38-linux_x86_64.whl
  #+end_src

  Are we good yet? Not exactly. Our wheel's platform tag is still
  ~linux_x86_64~ as opposed to something based on ~manylinux~. The
  platform tag is important because when ~pip~ goes to look for wheels
  to install, the platform tag is what is helping it choose the right
  version to download and install.

  The attribution of the ~manylinux~ platform tag is not the job of
  ~pip~, but it is ~auditwheel~'s. This utility scans the wheel and
  decides whether or not it can be attributed a ~manylinux~ tag. If
  yes, it creates a new wheel with the correct name tag.

  Let's first inspect out wheel - this only prints info, doest not
  create a new wheel yet.

  #+begin_example
    [root@e42ba33f35c4 io]# auditwheel show septembrse-0.0.0-cp38-cp38-linux_x86_64.whl

    septembrse-0.0.0-cp38-cp38-linux_x86_64.whl is consistent with the
    following platform tag: "linux_x86_64".

    The wheel references external versioned symbols in these
    system-provided shared libraries: libgcc_s.so.1 with versions
    {'GCC_3.0', 'GCC_3.3', 'GCC_4.2.0', 'GCC_4.3.0', 'GCC_3.3.1'},
    libc.so.6 with versions {'GLIBC_2.3', 'GLIBC_2.3.4', 'GLIBC_2.10',
    'GLIBC_2.14', 'GLIBC_2.4', 'GLIBC_2.2.5', 'GLIBC_2.17'},
    libstdc++.so.6 with versions {'CXXABI_1.3.3', 'GLIBCXX_3.4.18',
    'CXXABI_1.3', 'GLIBCXX_3.4', 'CXXABI_1.3.2', 'CXXABI_1.3.5'},
    libgfortran.so.3 with versions {'GFORTRAN_1.0'}, libm.so.6 with
    versions {'GLIBC_2.2.5'}, libquadmath.so.0 with versions
    {'QUADMATH_1.0'}

    This constrains the platform tag to "manylinux_2_17_x86_64". In order
    to achieve a more compatible tag, you would need to recompile a new
    wheel from source on a system with earlier versions of these
    libraries, such as a recent manylinux image.
  #+end_example

  The important information is that our wheel is valid for the
  platform tag ~manylinux_2_17_x86_64~. This means it is expected to
  work on any GNU/Linux system with a version of glibc equal or above
  2.17. That's expected because the version of glibc in this Docker
  image is 2.17.

  To actually produce the manylinux wheel, we use the  ~auditwheel repair~ command:
  #+begin_src shell
    auditwheel repair septembrse-0.0.0-cp38-cp38-linux_x86_64.whl
  #+end_src

  A new direcoty ~wheelhouse~ was created with out manylinux wheel in it.

** Runtime dependency on OpenBLAS

   There's is one detail I glossed over.

   Our C extension module has dynamics dependencies to various shared
   libraries.

   #+begin_src shell
     root@221b30d4d160:/io# ldd example_pkg/example.cpython-38-x86_64-linux-gnu.so
	     linux-vdso.so.1 (0x00007ffd2dfed000)
	     libopenblas.so.0 => /usr/lib/libopenblas.so.0 (0x00007ff591260000)
	     libstdc++.so.6 => /usr/lib/x86_64-linux-gnu/libstdc++.so.6 (0x00007ff590ede000)
	     libm.so.6 => /lib/x86_64-linux-gnu/libm.so.6 (0x00007ff590bda000)
	     libgcc_s.so.1 => /lib/x86_64-linux-gnu/libgcc_s.so.1 (0x00007ff5909c3000)
	     libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007ff590624000)
	     libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007ff590407000)
	     libgfortran.so.3 => /usr/lib/x86_64-linux-gnu/libgfortran.so.3 (0x00007ff5900e1000)
	     /lib64/ld-linux-x86-64.so.2 (0x00007ff5935d1000)
	     libquadmath.so.0 => /usr/lib/x86_64-linux-gnu/libquadmath.so.0 (0x00007ff58fea2000)
   #+end_src

   Most of these dependencies are libraries that we would expect to be
   present on most GNU/Linux systems out there. But this is not the
   case of ~libopenblas~ - which we installed manually as a build-time
   dependency inside the /manylinux/ Docker image. In the current
   state of things, a user could install the wheel fine, but at the
   moment they would import the package it would break: ~libopenblas
   not found~!

   The solution to this is simple: let's add ~libopenblas~ to the
   wheel (remember, a wheel is nothing else than an archive). This is,
   however, not enough. We also need to make sure that, at runtime,
   the system knows where to look to find this shared library. This
   can be done by modyfing the ~DT_RUNPATH~ list of directories in the
   extension module shared library. This is rather technical and
   error-prone, but ~auditwheel repair~ does the work for us. Prior to
   creating the new /manylinux/ wheel, this command
   1. Scans the dynamics dependencies for the wheel's extension
      module(s)s and identifies those that are outside of a very
      restricited set of shared libraries usually distributed by most
      GNU/Linux distributions.
   2. Copies the corresponding shared libraries (~.so~ file) into the
      wheel.
   3. Modifies the ~DT_RUNPATH~ (or ~DT_RPATH~) entry for the compiled
      extension module(s) so that the dynamic linker finds these
      shares libraries at runtime.

   If use ~ldd~ on the ~example~ extension inside the manylinux wheel
   this time, we see that a few shared libraries are now found inside
   the wheel itself.

   We now have a self-contained wheel that is usable across many linux
   distributions.

   I want to point out that this runtime dependency issue isn't
   restricted to building wheels on GNU/Linux systems. On MacOS,
   [[https://github.com/matthew-brett/delocate][delocate]] does a job similar to ~auditwheel~. On Windows, I haven't
   found a way to embed dynamic dependecnies (~dll~ files) inside
   ~win32~ or ~win64~ wheels. An alternative is to link these
   dependencies statically.

* Conclusion

  That's all I wanted to show you today. If you're currently working
  on a Python project or plan to do so, then I really encourage you to
  dig a bit deeper into the topic. The material for this walthrough is
  available online, along with a list of references for further
  reading. I also encourage you distribute wheels for your projects,
  it will make it much easier to use your package. Building wheels is
  a relatively complex process for computers, but for us packagers not
  so much. That is thanks to the work of the PyPA who maintain
  manylinux and auditwheel, the contributors behing delocate and
  cibuildwheel. So let's thank them and happy wheel building!
